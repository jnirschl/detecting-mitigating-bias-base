{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "milestone 4 instructions",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feDRsCpVcFkP"
      },
      "source": [
        "# Milestone 4\n",
        "\n",
        "#### Objective:\n",
        "\n",
        " - Audit a model for bias when the protected class is not available.\n",
        "\n",
        "#### Workflow:\n",
        "\n",
        " - Introduction\n",
        " - Set up problem\n",
        " - Load data and display primary and secondary sets\n",
        " - Load model to audit for bias\n",
        " - Train model from proxy variables to predict the protected class\n",
        " - Assess the model bias\n",
        " - Report your findings\n",
        "\n",
        "#### Importance to Project:\n",
        "\n",
        " - Very often, protected class information is not available when you are auditing for bias or removing bias.  \n",
        " - In these cases, the task becomes much more challenging. \n",
        " - You might not have access to the protected class information either because you can only see the model itself and not the underlying data, or because even the original creators don't know the protected class status of each row. \n",
        " - While auditing or mitigating bias without direct access to the protected class introduces a level of uncertainty, we can still reach some conclusions about biased models, but must be open about the uncertainty.\n",
        " - The most powerful techniques are beyond the scope of this milestone. However, knowing some baseline techniques can be helpful in easy cases and can help guide a decision whether to pursue a more in-depth audit.\n",
        "\n",
        "#### Resources:\n",
        "\n",
        " - You should be able to proceed with this milestone without any additional resources. However, the following resources will provide useful context and more information.\n",
        "  - https://arxiv.org/pdf/1906.00285.pdf and accompanying video https://www.youtube.com/watch?v=z-ZI1VoA-O0]. Milestone 4 tracks this paper closely - the main difference is in methodology.\n",
        "  - https://arxiv.org/pdf/1811.11154.pdf - This paper demonstrates some of the weaknesses of proxy-based approaches\n",
        "  - https://github.com/cfpb/proxy-methodology - This repository contains the BISG proxy methodology\n",
        "  - https://amstat.tandfonline.com/doi/full/10.1080/2330443X.2018.1427012 - This paper explains the BISG proxy methodology"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jNuOpjrcFkP"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7vX-MgMcFkP"
      },
      "source": [
        "We can still assess algorithmic fairness when the protected class is unobserved, provided we have some way to deduce the protected class from the available data. The most famous real-world example of this is the BISG proxy, which uses a Naive Bayes model to predict race membership based on name and zipcode. As [Chen et al](https://arxiv.org/pdf/1811.11154.pdf) note, \"This methodology notably supported a $98 million fine against a major auto loan lender\".\n",
        "\n",
        "While the BISG proxy methodology is still used and developed, recent scholarship in machine fairness demonstrates some serious shortcomings and some new directions for modelling without access to protected class data. The fundamental issue is that proxy models, unless highly accurate, could skew the audit. For instance, [Chen et al](https://arxiv.org/pdf/1811.11154.pdf) showed the BISG methodology has been shown to overestimate true disparities. In summary, we need to exercise great care when building proxy models and imputing protected class data.\n",
        "\n",
        "In terms of new directions for assessing algorithmic fairness when the protected class is unobserved, an interesting paper released this year entitled [\"Assessing Algorithmic Fairness with Unobserved Protected Class Using Data Combination\"](https://arxiv.org/pdf/1906.00285.pdf) describes a methodology for estimating confidence intervals for bias metrics. While the methodology is too complex to review here, in the course of this milestone it will become clear why confidence intervals are so useful. It is well worth considering for any bias audit where the protected class is unavailable.\n",
        "\n",
        "Regardless of the chosen methodology, the general setup requires the following datasets:\n",
        " - a primary dataset, which contains input variables to the model and the model's final prediction\n",
        " - a secondary dataset, which contains some proxy variables which overlap with the primary dataset, and does contain the protected class labels\n",
        "     \n",
        "An example of a primary set and secondary set in a lending context with potential racial bias can be found below (proxy variables are surname and zipcode, as in BISG, and they can be found in both the primary and secondary sets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsw2mDgOcFkP"
      },
      "source": [
        "## Setting up the problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZsOLhPEcFkP"
      },
      "source": [
        "In our example, you are asked by the WHO to evaluate bias in a model which determines personalized warfarin dosage. A similar model built on the same dataset has already been audited using state-of-the-art partial identification sets in a recent paper by Kallus, Mao and Zhou. We will be using a simpler methodology.\n",
        "\n",
        "Warfarin is the world's most common anticoagulant, but the correct dosage for the desired effect changes significantly from person to person. Too little warfarin will fail to produce a coagulant effect, while too much warfarin can cause dangerous bleeding, stroke and death. \n",
        "\n",
        "In these circumstances, dosage by trial and error is dangerous, so the medical community has built models to predict the ideal warfarin dosage. These models can be found in section 8.2 of [\"Assessing Algorithmic Fairness with Unobserved Protected Class Using Data Combination\"](https://arxiv.org/pdf/1906.00285.pdf). The makers of the models note that genetic factors correlated with race are predictive of the outcome, so we can expect dosage and dosage prediction to differ by race. In this scenario, the WTO has asked you to audit the model's bias with respect to race. \n",
        "\n",
        "For this audit, we are going to simulate an environment where race data is not available. To audit the model's bias with respect to race, one method is to train a proxy model on a primary set as described above. Instead of approaching the problem blindly, you are given two categories of columns: genetic features, and medication features. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoxoagJscFkP"
      },
      "source": [
        "## Load libraries and dataset\n",
        "\n",
        " - The following code will load all necessary libraries and data. \n",
        " - The code should load:\n",
        "   - A primary and secondary dataset\n",
        "   - An initial model\n",
        "   - A set of genotype columns and a set of medication columns, which can be each used to train a proxy\n",
        "   - Columns initially used to train the model\n",
        "   - A true positive rate helper function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wfybqsXcFkP"
      },
      "source": [
        "import pandas as pd\n",
        "import pickle\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import sklearn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wR1NYsrDcFkQ"
      },
      "source": [
        "%run \"./Milestone 4 declarations.ipynb\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19Jp6ISXcFkQ"
      },
      "source": [
        "print('Primary dataset')\n",
        "display(primary)\n",
        "print('Secondary dataset')\n",
        "display(secondary)\n",
        "print('Confirm model loaded')\n",
        "print('Genotype columns:\\n', genotype_columns,'\\n\\n')\n",
        "print('Medication columns:\\n', medication_columns,'\\n\\n')\n",
        "print('Model training columns:\\n', training_columns,'\\n\\n')\n",
        "print(help(tpr))\n",
        "model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGDYPmF4cFkQ"
      },
      "source": [
        "## Given the supplied training columns, use the model to predict warfarin dosage for the primary set\n",
        "\n",
        " - If loaded properly, you can just use model.predict on the training columns of the primary dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyS-H0DxcFkQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNZhI9A2cFkQ"
      },
      "source": [
        "## Train a proxy model on the secondary set to impute the missing protected class data\n",
        "\n",
        " - The overall goal of this section is to train some proxy models to predict race.\n",
        " - The only requirement for a proxy model is that its inputs are available in both primary and secondary sets, and that it predicts the column we are trying to impute (race). A simple Random Forest or LightGBM classifier, trained on the secondary set, will suffice. **Make sure you hold out a test portion of the secondary set to test performance.**\n",
        " - Based on the column lists approved by domain experts (medication_columns, genotype_columns), three models are possible: one model uses medication_columns to predict race, one uses genotype_columns to predict race, and one uses both.\n",
        " - Create a confusion matrix showing each proxy model's performance on the same test set\n",
        " - Conclude which model has the best performance. We will use it going forward."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPVGRNO8cFkQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVsgntNBcFkQ"
      },
      "source": [
        "## Add a race column to the primary dataset by using the proxy model with the best performance\n",
        "\n",
        "Since the proxy columns are present in the primary and secondary sets, you can now predict the race of each row in the primary dataset. Simply run proxy_model.predict on the column set you chose in the primary set.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKo4J0lIcFkQ"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADaEyTClcFkQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "it24t6NNcFkQ"
      },
      "source": [
        "##  Assess the model bias with the best proxy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1Q34zo5cFkQ"
      },
      "source": [
        "In a real-world context, we will usually have some idea of what specific bias metrics we want to measure and report back on. In this case, we can take some guidance from [\"Assessing Algorithmic Fairness with Unobserved Protected Class Using Data Combination\"](https://arxiv.org/pdf/1906.00285.pdf) paper and look to the bias metrics they investigated for this dataset. The authors in that case study were interested in the distribution of high dosage recommendations by race (\"demographic disparity\", see section 8.2 of [\"Assessing Algorithmic Fairness with Unobserved Protected Class Using Data Combination\"](https://arxiv.org/pdf/1906.00285.pdf)), and the distribution of true positive rates by race (\"equal opportunity difference\").\n",
        "\n",
        " - Grouping the dataset by race, observe how the high dosage model's recommendations are distributed, and how they compare to the ground truth ('target'). Is there a disparity in dosage recommendations by race (**disparate impact**)?\n",
        " - Using the tpr helper function, compare the true positive rates for each race. Is there a disparity in TPRs (**equal opportunity difference**)? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10W1h9tRcFkQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUFsqk1mcFkQ"
      },
      "source": [
        "## Report on intermediate findings\n",
        "\n",
        " - Were the high dosage recommendation rates roughly equivalent across races, or was there a substantial difference indicating disparate impact?\n",
        " - Were the true positive rates roughly equivalent across races, or was there a substantial difference indicating equal opportunity difference?\n",
        " - What are some reasons to be cautious about true positive rates calculated with the help of a proxy model?\n",
        " - What actions could be taken by data scientists or clinicians as a result of your findings?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yyPAUzJcFkQ"
      },
      "source": [
        "##  Assess the model bias using race imputation from other proxies\n",
        " - Since we have other proxy models available, we can gain a little more confidence in our findings by seeing consistent results when other proxies are used.\n",
        " - Repeat your investigation for high dosage recommendation rate disparities using race imputation from the other proxies. Confirm whether the insights align.\n",
        " - Repeat your investigation for TPR disparities using race imputation from the other proxies. Confirm whether the insights align."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAol1TQrcFkQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DY3s1i-fcFkQ"
      },
      "source": [
        "## Complete the deliverable\n",
        "\n",
        "Produce a memo which answers the following questions, taking into account the purpose of the model (assessing the risk of cardiovascular disease)\n",
        "\n",
        " - Is there a disparity in true positive rates along racial lines?\n",
        " - How could this disparity constitute unwanted bias? What are some possible real-world effects?\n",
        " - What are some reasons to be cautious about true positive rates calculated with the help of a proxy model?\n",
        " - What actions could be taken by data scientists or clinicians as a result of your findings?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKWjWevncFkQ"
      },
      "source": [
        "## Reflect on the importance of having a highly accurate proxy when modelling without access to protected class data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szR7YyPVcFkQ"
      },
      "source": [
        ""
      ]
    }
  ]
}